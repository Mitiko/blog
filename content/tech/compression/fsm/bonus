Static models don't require per-symbol updates and they're much faster, but they're far from optimal in terms of
<abbr title="Compression ratio">CR</abbr>.  
Instead, static models are given on a per-block-basis.
Most commonly, this is in the form of a [prediction table](https://en.wikipedia.org/wiki/Deflate#Stream_format)
that gets encoded in the compressed file.  
It is also possible to train an adaptive model on the previous block, and use it as a static one
in the following block but this probably defeats the purpose of most use-cases.

> If you need speed, go static!


Another cool feature is you can train a hyper-good adaptive model on a certain file and then use it as a static one.
This gives you high decompression speed with better ratios, but it's sensitive to how well you serialize the model.
-> EXAMPLES!!

A special case of the static model is the no-model:
```rust
// Bitwise case
fn get_prob(&self) -> u16 {
    // represents 1/2
    1 << 15
}
```
If the model has a [uniform distribution](https://en.wikipedia.org/wiki/Discrete_uniform_distribution), then it's equivalent
to not doing any entropy coding at all.
Examples of such coders are: TODO: Put examples here


Mostly all compressors process data byte by byte in order, maybe with the exception of
[BWT pre-processing](https://en.wikipedia.org/wiki/Burrows%E2%80%93Wheeler_transform)
or in image compression when 8x8 [DCTs](https://en.wikipedia.org/wiki/Discrete_cosine_transform)
get [zig-zagged](https://en.wikipedia.org/wiki/JPEG#Entropy_coding).

There're two (main) ways to make predictors:
- some static obscure algorithm
- adaptive models

Initially when computers took off (and also had much less computational power),
the default was to transmit the statistics for each symbol for each block.
In fact, [Huffman coding](https://en.wikipedia.org/wiki/Huffman_coding) inherently uses a static
probability table.

It's clear there exist non-stationary data sources, the canonical example being a dictionary -
in the beginning you can expect to see a lot of words start with `a` but as you move forward
this distribution gets more and more skewed.

So we turned to the best predictor we have, **THE BRAIN** (*evolutionary proven*&trade;).
In fact, the brain is so complex that we're having a better chance at exploring how it works
by building our own silicone based ones and comparing it with the real thang.

<div id="anchor"></div>

The reason the brain is so good at prediction is because it's very good at understanding context.
You are capable of expecting (*predicting*) different outcomes (*symbol distributions*) in
different situations (*contexts*).  
The only problem is... there's waayyy to much data in the real world. Hence [sensory overload](https://en.wikipedia.org/wiki/Sensory_overload).
Luckily the brain has figured out how to eliminate much of the noise (*aka lossy compression*).

> The main assumption being made is future events will have similiar outcomes as past events.  
> This is the basis of all physics, of life, and our universe - **determinism**.

Adaptive predictors do something similiar - for a given context, they assume a probability distribution.
With each new observation from a given context, they adapt the distribution (for that context) a little.
Modern state-of-art neural nets have also adapted this standard and switch behavior based on context (see [transformers](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))).

